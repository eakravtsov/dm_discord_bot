import os
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEndpoint


class LlmManager:
    """
    A simple manager to interact with a remote Hugging Face LLM using an API key.

    This class uses the Hugging Face Inference API to get responses from a model,
    so no local model download is required.
    """

    def __init__(self):
        """
        Initializes the Hugging Face API client.

        It requires the HUGGINGFACEHUB_API_TOKEN environment variable to be set.
        """
        # A popular and powerful model available through the API.
        # You can change this to any model ID on the Hugging Face Hub.
        model_id = "mistralai/Mistral-7B-Instruct-v0.2"
        load_dotenv()

        # Get the API token from environment variables
        api_token = os.getenv("HUGGING_FACE_TOKEN")
        if not api_token:
            raise ValueError(
                "HUGGING_FACE_TOKEN environment variable not set. "
                "Please get a token from https://huggingface.co/settings/tokens"
            )

        print(f"Initializing remote LLM endpoint for model: {model_id}")

        # Initialize the HuggingFaceEndpoint for remote inference.
        self.llm = HuggingFaceEndpoint(
            repo_id=model_id,
            huggingfacehub_api_token=api_token,
            max_new_tokens=250,
            temperature=0.7,
        )
        print("âœ… Remote LLM endpoint initialized.")

    def get_llm_response(self, user_message: str) -> str:
        """
        Gets a response from the remote LLM for a given message.

        Args:
            user_message: The text input from the user.

        Returns:
            The text response generated by the model.
        """
        print(f"ðŸ§  Sending prompt to remote LLM: '{user_message}'")

        # 'invoke' calls the remote API
        response = self.llm.invoke(user_message)

        print(f"ðŸ¤– LLM responded: '{response}'")
        return response


# This block demonstrates how to use the LlmManager class
if __name__ == '__main__':
    # Create an instance of the manager
    # This will fail if the environment variable is not set
    try:
        llm_manager = LlmManager()

        # Define a sample prompt
        prompt = "Explain the importance of the Magna Carta in one paragraph."

        # Get the response
        response = llm_manager.get_llm_response(prompt)

        # Print the final result
        print("\n--- Final Answer ---")
        print(f"Question: {prompt}")
        print(f"Answer: {response}")
        print("--------------------")

    except ValueError as e:
        print(f"ðŸ›‘ ERROR: {e}")

